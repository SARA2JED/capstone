{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load data \n",
    "song_data = pd.read_csv(\"clean_file.csv\")\n",
    "df = pd.DataFrame(song_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i could feel time there way know fallen leav night who say theyr blow a free wind hope learn whi sea tide ha way turn more thi you know there noth more thi tell one thing more thi you know there noth it fun while there way know like dream night who say were go no care world mayb im learn whi sea tide ha way turn more thi you know there noth more thi tell one thing more thi you know there noth more thi you know there noth more thi tell one thing more thi there noth'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% test \n",
    "df['processed_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df['processed_text'].iloc[:10000]))\n",
    "\n",
    "#data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could', 'feel', 'time', 'there', 'way', 'know', 'fallen', 'leav', 'night', 'who', 'say', 'theyr', 'blow', 'free', 'wind', 'hope', 'learn', 'whi', 'sea', 'tide', 'ha', 'way', 'turn', 'more', 'thi', 'you', 'know', 'there', 'noth', 'more', 'thi', 'tell', 'one', 'thing', 'more', 'thi', 'you', 'know', 'there', 'noth', 'it', 'fun', 'while', 'there', 'way', 'know', 'like', 'dream', 'night', 'who', 'say', 'were', 'go', 'no', 'care', 'world', 'mayb', 'im', 'learn', 'whi', 'sea', 'tide', 'ha', 'way', 'turn', 'more', 'thi', 'you', 'know', 'there', 'noth', 'more', 'thi', 'tell', 'one', 'thing', 'more', 'thi', 'you', 'know', 'there', 'noth', 'more', 'thi', 'you', 'know', 'there', 'noth', 'more', 'thi', 'tell', 'one', 'thing', 'more', 'thi', 'there', 'noth']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['feel', 'time', 'way', 'know', 'fall', 'leav', 'night', 'say', 'theyr', 'blow', 'free', 'wind', 'hope', 'learn', 'whi', 'tide', 'turn', 'thi', 'know', 'tell', 'thing', 'thi', 'know', 'way', 'know', 'dream', 'night', 'go', 'care', 'world', 'm', 'learn', 'whi', 'tide', 'turn', 'thi', 'know', 'tell', 'thing', 'thi', 'know', 'know', 'tell', 'thing', 'thi']]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    " \n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    " \n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    " \n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 7), (9, 2), (10, 1), (11, 1), (12, 2), (13, 1), (14, 3), (15, 1), (16, 5), (17, 3), (18, 2), (19, 1), (20, 2), (21, 2), (22, 2), (23, 1), (24, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blow'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% word assigned to 0 \n",
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('blow', 1),\n",
       "  ('care', 1),\n",
       "  ('dream', 1),\n",
       "  ('fall', 1),\n",
       "  ('feel', 1),\n",
       "  ('free', 1),\n",
       "  ('go', 1),\n",
       "  ('hope', 1),\n",
       "  ('know', 7),\n",
       "  ('learn', 2),\n",
       "  ('leav', 1),\n",
       "  ('m', 1),\n",
       "  ('night', 2),\n",
       "  ('say', 1),\n",
       "  ('tell', 3),\n",
       "  ('theyr', 1),\n",
       "  ('thi', 5),\n",
       "  ('thing', 3),\n",
       "  ('tide', 2),\n",
       "  ('time', 1),\n",
       "  ('turn', 2),\n",
       "  ('way', 2),\n",
       "  ('whi', 2),\n",
       "  ('wind', 1),\n",
       "  ('world', 1)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% print words and thier freq  \n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.489*\"get\" + 0.168*\"gon\" + 0.071*\"ai\" + 0.036*\"work\" + 0.022*\"well\" + 0.021*\"move\" + 0.018*\"everybodi\" + 0.016*\"fool\" + 0.012*\"floor\" + 0.012*\"comin\"'), (1, '0.381*\"time\" + 0.148*\"long\" + 0.129*\"wait\" + 0.127*\"run\" + 0.085*\"last\" + 0.040*\"goodby\" + 0.016*\"lover\" + 0.010*\"past\" + 0.010*\"piti\" + 0.009*\"shot\"'), (2, '0.144*\"soul\" + 0.089*\"noth\" + 0.067*\"nt\" + 0.059*\"blood\" + 0.058*\"follow\" + 0.054*\"beat\" + 0.036*\"babe\" + 0.034*\"bodi\" + 0.033*\"stone\" + 0.033*\"whatev\"'), (3, '0.226*\"blue\" + 0.150*\"burn\" + 0.057*\"flame\" + 0.050*\"wide\" + 0.048*\"summer\" + 0.038*\"window\" + 0.033*\"lock\" + 0.032*\"dig\" + 0.031*\"bind\" + 0.031*\"imagin\"'), (4, '0.362*\"come\" + 0.099*\"home\" + 0.072*\"call\" + 0.064*\"talk\" + 0.048*\"sing\" + 0.039*\"song\" + 0.038*\"bring\" + 0.031*\"town\" + 0.021*\"write\" + 0.019*\"bit\"'), (5, '0.106*\"night\" + 0.078*\"girl\" + 0.054*\"light\" + 0.046*\"rock\" + 0.035*\"woman\" + 0.031*\"kiss\" + 0.030*\"sweet\" + 0.030*\"roll\" + 0.029*\"side\" + 0.025*\"name\"'), (6, '0.169*\"watch\" + 0.154*\"rain\" + 0.125*\"bad\" + 0.124*\"dark\" + 0.067*\"goe\" + 0.060*\"pray\" + 0.023*\"remain\" + 0.022*\"thunder\" + 0.015*\"undon\" + 0.014*\"hungri\"'), (7, '0.070*\"say\" + 0.067*\"see\" + 0.047*\"do\" + 0.047*\"tell\" + 0.040*\"need\" + 0.037*\"think\" + 0.032*\"ill\" + 0.026*\"hear\" + 0.024*\"well\" + 0.024*\"keep\"'), (8, '0.300*\"make\" + 0.150*\"good\" + 0.133*\"babi\" + 0.063*\"lone\" + 0.055*\"round\" + 0.052*\"boy\" + 0.027*\"chanc\" + 0.025*\"sad\" + 0.022*\"scream\" + 0.016*\"ladi\"'), (9, '0.215*\"eye\" + 0.193*\"dream\" + 0.105*\"end\" + 0.083*\"close\" + 0.067*\"sleep\" + 0.047*\"rise\" + 0.044*\"morn\" + 0.042*\"wake\" + 0.029*\"stori\" + 0.023*\"beauti\"'), (10, '0.156*\"stand\" + 0.115*\"fire\" + 0.076*\"today\" + 0.057*\"fli\" + 0.046*\"broken\" + 0.032*\"mother\" + 0.032*\"war\" + 0.028*\"steal\" + 0.028*\"wear\" + 0.023*\"next\"'), (11, '0.060*\"day\" + 0.049*\"look\" + 0.036*\"world\" + 0.036*\"life\" + 0.034*\"live\" + 0.031*\"man\" + 0.026*\"much\" + 0.025*\"right\" + 0.024*\"play\" + 0.020*\"friend\"'), (12, '0.483*\"want\" + 0.298*\"give\" + 0.123*\"wanna\" + 0.016*\"everyday\" + 0.007*\"candi\" + 0.005*\"wet\" + 0.002*\"grey\" + 0.001*\"lust\" + 0.001*\"tast\" + 0.000*\"real\"'), (13, '0.395*\"know\" + 0.202*\"feel\" + 0.154*\"way\" + 0.120*\"thing\" + 0.046*\"whi\" + 0.022*\"fine\" + 0.010*\"pretend\" + 0.008*\"fate\" + 0.003*\"fail\" + 0.003*\"hook\"'), (14, '0.201*\"love\" + 0.157*\"go\" + 0.083*\"take\" + 0.081*\"let\" + 0.054*\"away\" + 0.045*\"back\" + 0.036*\"find\" + 0.034*\"hold\" + 0.033*\"lose\" + 0.032*\"fall\"'), (15, '0.163*\"never\" + 0.135*\"heart\" + 0.063*\"tonight\" + 0.062*\"break\" + 0.056*\"lie\" + 0.041*\"stop\" + 0.036*\"die\" + 0.036*\"cri\" + 0.036*\"true\" + 0.027*\"forev\"'), (16, '0.095*\"bear\" + 0.065*\"send\" + 0.063*\"hous\" + 0.054*\"land\" + 0.051*\"water\" + 0.050*\"hang\" + 0.038*\"mad\" + 0.037*\"son\" + 0.037*\"read\" + 0.032*\"drop\"'), (17, '0.177*\"togeth\" + 0.177*\"big\" + 0.164*\"music\" + 0.096*\"number\" + 0.060*\"radio\" + 0.058*\"band\" + 0.032*\"problem\" + 0.028*\"record\" + 0.018*\"enjoy\" + 0.008*\"trail\"'), (18, '0.449*\"m\" + 0.115*\"ve\" + 0.069*\"caus\" + 0.061*\"alway\" + 0.044*\"plea\" + 0.026*\"ask\" + 0.025*\"help\" + 0.014*\"co\" + 0.012*\"believ\" + 0.012*\"someday\"'), (19, '0.274*\"show\" + 0.149*\"fight\" + 0.132*\"save\" + 0.069*\"pay\" + 0.068*\"fast\" + 0.056*\"treat\" + 0.044*\"becom\" + 0.029*\"twist\" + 0.025*\"month\" + 0.015*\"neck\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 20 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff1de33649510ac30c93f6edf1fdd6efd6fa4433b0a5d3feb7f2587c455ddc0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
